{"cells":[{"cell_type":"markdown","metadata":{"id":"ChC3RF8meAlK"},"source":["<h1 align=\"center\">An Introduction to Machine Learning - 25737</h1>\n","<h4 align=\"center\">Dr. Yassaee</h4>\n","<h4 align=\"center\">Sharif University of Technology, Autumn 2024</h4>\n","\n","**Student Name**:\n","\n","**Student ID**:"]},{"cell_type":"markdown","metadata":{"id":"IraiR0SbeDi_"},"source":["# Logistic Regression"]},{"cell_type":"markdown","source":["Logistic regression is a **supervised machine learning algorithm** primarily used for **binary classification tasks**. It predicts the probability of an outcome belonging to one of two classes (0 or 1) using a logistic function, specifically the **sigmoid function**, which maps any real-valued number into the range [0, 1].\n","\n","### Key Features:\n","- **Binary Outcomes**: Logistic regression is suitable when the dependent variable is binary, such as yes/no or pass/fail.\n","- **Maximum Likelihood Estimation (MLE)**: Coefficients are estimated using MLE to maximize the likelihood of the observed data.\n","- **Interpretation**: The output can be interpreted as odds ratios, indicating how changes in independent variables affect the likelihood of an event.\n","\n","### Types:\n","1. **Binary Logistic Regression**: Two possible outcomes.\n","2. **Multinomial Logistic Regression**: More than two categories.\n","3. **Ordinal Logistic Regression**: Ordered categories.\n","\n","Logistic regression is widely used in fields like finance, healthcare, and social sciences for tasks such as risk assessment and disease diagnosis."],"metadata":{"id":"lwFjioTPW4G6"}},{"cell_type":"markdown","metadata":{"id":"nRQjwWC3eDnc"},"source":["**Task:** Implement your own Logistic Regression model, and test it on the given dataset of Logistic_question.csv!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VwIHOWAHVA4c"},"outputs":[],"source":["# import necessary libraries\n","\n","class MyLogisticRegression:\n","    class MyLogisticRegression:\n","    # TODO: Initialize the class with necessary attributes, such as learning rate, number of iterations, and parameters (weights and bias).\n","    def __init__(self, learning_rate=0.01, num_iterations=1000):\n","        pass\n","\n","    # TODO: Implement a loss function for logistic regression, using binary cross-entropy as the loss metric.\n","    def loss(self, y_true, y_pred):\n","        pass\n","\n","    # TODO: Implement the fit method to train the model. Use gradient descent to update the weights and bias based on the training data.\n","    # Ensure that the code is optimized to run on GPU if available (e.g., by using torch tensors on GPU).\n","    def fit(self, X, y):\n","        pass\n","\n","    # TODO: Implement the predict function that uses the learned weights and bias to output predictions on new data.\n","    # Apply a sigmoid function and set a threshold (e.g., 0.5) to determine the binary outcome.\n","    def predict(self, X):\n","        pass"]},{"cell_type":"markdown","metadata":{"id":"S-i-oubUlZ6e"},"source":["**Task:** Test your model on the given dataset. You must split your data into train and test, with a 0.2 split, then normalize your data using X_train data. Finally, report 4 different evaluation metrics of the model on the test set. (You might want to first make the Target column binary!)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0KXzIy_2u-pG"},"outputs":[],"source":["\n","# TODO: Load the dataset from Logistic_question.csv.\n","# Make sure to import necessary libraries for loading and handling CSV data (e.g., pandas, numpy).\n","\n","# TODO: Make the 'Target' column binary if necessary.\n","# For example, you could map specific values to 0 and 1 based on conditions.\n","\n","# TODO: Split the data into features (X) and target (y).\n","# Then, split the data into training and testing sets with an 80-20 split.\n","\n","# TODO: Normalize the training data (X_train) and use the same scaling parameters to normalize X_test.\n","\n","# TODO: Create an instance of MyLogisticRegression.\n","# Fit the model on the training set (X_train, y_train).\n","\n","# TODO: Predict the target values on the test set (X_test).\n","\n","# TODO: Calculate and print 4 different evaluation metrics on the test set predictions.\n","# Suggested metrics: accuracy, precision, recall, and F1-score.\n","# You may want to import these metrics from sklearn (e.g., accuracy_score, precision_score, recall_score, f1_score)."]},{"cell_type":"markdown","metadata":{"id":"Ji0RXNGKv1pa"},"source":["**Question:** What are each of your used evaluation metrics? And for each one, mention situations in which they convey more data on the model performance in specific tasks."]},{"cell_type":"markdown","metadata":{"id":"ldveD35twRRZ"},"source":["**Your answer:**"]},{"cell_type":"markdown","metadata":{"id":"1ZCeRHZSw-mh"},"source":["**Task:** Now test the built-in function of Python for Logistic Regression, and report all the same metrics used before."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vb5lRSQXDLR3"},"outputs":[],"source":["# TODO: Create an instance of LogisticRegression from scikit-learn.\n","\n","# TODO: Fit the built-in Logistic Regression model on the training set (X_train, y_train).\n","\n","# TODO: Use the trained model to predict on the test set (X_test).\n","\n","# TODO: Calculate and print the same evaluation metrics as before (e.g., accuracy, precision, recall, F1-score)\n","# to compare the performance of the custom model with the built-in Logistic Regression."]},{"cell_type":"markdown","metadata":{"id":"RCvIymmMy_ji"},"source":["**Question:** Compare your function with the built-in function. On the matters of performance and parameters. Briefly explain what the parameters of the built-in function are and how they affect the model's performance?"]},{"cell_type":"markdown","metadata":{"id":"EY0ohM16z3De"},"source":["**Your answer:**"]},{"cell_type":"markdown","metadata":{"id":"ClMqoYlr2kr7"},"source":["# Multinomial Logistic Regression"]},{"cell_type":"markdown","source":["Multinomial logistic regression is a statistical method used for **classifying outcomes** when the dependent variable has **more than two categories**. Unlike binary logistic regression, which handles only two possible outcomes, multinomial logistic regression can predict the probabilities of multiple discrete outcomes based on one or more independent variables, which can be continuous or categorical.\n","\n","### Key Features:\n","- **Generalization of Logistic Regression**: It extends the binary logistic model to handle multiclass problems, making it suitable for scenarios like predicting consumer preferences or classifying types of jobs.\n","- **Probability Estimation**: The model estimates the probability of each category by calculating a score for each potential outcome and applying the softmax function to convert these scores into probabilities.\n","- **Modeling Approach**: It can be conceptualized as running multiple binary logistic regressions, where one category is treated as a reference (or pivot) against which others are compared."],"metadata":{"id":"2YOQIMPXX_UN"}},{"cell_type":"markdown","metadata":{"id":"ukvlqDe52xP5"},"source":["**Task:** Implement your own Multinomial Logistic Regression model. Your model must be able to handle any number of labels!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Ir-_hFt286t"},"outputs":[],"source":["class MyMultinomialLogisticRegression:\n","    # TODO: Initialize the class with necessary attributes, such as learning rate, number of iterations, and parameters (weights and bias).\n","    # Remember that, for multinomial logistic regression, weights will be a matrix with shape (n_features, n_classes).\n","    def __init__(self, learning_rate=0.01, num_iterations=1000, n_classes=None):\n","        pass\n","\n","    # TODO: Implement a loss function for multinomial logistic regression, using categorical cross-entropy as the loss metric.\n","    # This will calculate the average log loss across all classes for a given set of predictions.\n","    def loss(self, y_true, y_pred):\n","        pass\n","\n","    # TODO: Implement the fit method to train the model. Use gradient descent to update the weights and bias based on the training data.\n","    # Ensure that the code is optimized to run on GPU if available (e.g., by using torch tensors on GPU).\n","    def fit(self, X, y):\n","        pass\n","\n","    # TODO: Implement the predict function that uses the learned weights and bias to output predictions on new data.\n","    # Use the softmax function to generate probabilities for each class, and assign each sample to the class with the highest probability.\n","    def predict(self, X):\n","        pass"]},{"cell_type":"markdown","metadata":{"id":"zPQ3Rtay3Y2_"},"source":["**Task:** Test your model on the given dataset. Do the same as the previous part, but here you might want to first make the Target column quantized into $i$ levels. Change $i$ from 2 to 10."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9aP4QJPq29B3"},"outputs":[],"source":["# TODO: Load the dataset.\n","# Make sure to import necessary libraries for handling CSV data (e.g., pandas, numpy).\n","\n","# TODO: Quantize the 'Target' column into `i` levels, where `i` ranges from 2 to 10.\n","# This means dividing the target values into `i` evenly spaced bins and assigning each value to a bin index (label).\n","\n","# TODO: For each value of `i` (from 2 to 10):\n","#       - Quantize the target column based on the current `i` value.\n","#       - Split the data into features (X) and quantized target (y).\n","#       - Split the data into training and testing sets with an 80-20 split.\n","#       - Normalize the training data (X_train) and use the same scaling parameters to normalize X_test.\n","\n","# TODO: Create an instance of MyMultinomialLogisticRegression.\n","# Fit the model on the training set (X_train, y_train).\n","\n","# TODO: Predict the target values on the test set (X_test).\n","\n","# TODO: Calculate and print 4 different evaluation metrics on the test set predictions.\n","# Suggested metrics: accuracy, precision (macro-averaged), recall (macro-averaged), and F1-score (macro-averaged).\n","# You may want to import these metrics from sklearn (e.g., accuracy_score, precision_score, recall_score, f1_score)."]},{"cell_type":"markdown","metadata":{"id":"Of2sHl5Z4dXi"},"source":["**Question:** Report for which $i$ your model performs best. Describe and analyze the results! You could use visualizations or any other method!"]},{"cell_type":"markdown","metadata":{"id":"cRLERDAr4wnS"},"source":["**Your answer:**"]},{"cell_type":"markdown","metadata":{"id":"wT43jGKV6CBZ"},"source":["# Going a little further!(bonus*)"]},{"cell_type":"markdown","metadata":{"id":"Vo9uGo0R6GZo"},"source":["First we download Adult income dataset from Kaggle! In order to do this create an account on this website, and create an API. A file named kaggle.json will be downloaded to your device. Then use the following code:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o-vrjYBF7u1E"},"outputs":[],"source":["from google.colab import files\n","files.upload()  # Use this to select the kaggle.json file from your computer\n","!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json"]},{"cell_type":"markdown","metadata":{"id":"5i6u6_1v8ftX"},"source":["Then use this code to automatically download the dataset into Colab."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XjyVaVKF29Hx"},"outputs":[],"source":["!kaggle datasets download -d wenruliu/adult-income-dataset\n","!unzip /content/adult-income-dataset.zip"]},{"cell_type":"markdown","metadata":{"id":"EXQnbZwt8rJK"},"source":["**Task:** Determine the number of null entries!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JtuEx6QW29c1"},"outputs":[],"source":["\n","# TODO: Import the pandas library to handle data manipulation and analysis.\n","\n","# TODO: Load the dataset into a pandas DataFrame.\n","\n","# TODO: Check for null entries in each column using the .isnull().sum() method.\n","# This will give the number of null entries for each column.\n","\n","# TODO: Calculate the total number of null entries in the dataset by summing the null values across all columns.\n","\n","# TODO: Print the number of null entries per column and the total number of null entries in the dataset."]},{"cell_type":"markdown","metadata":{"id":"JpEcBdTUAYVN"},"source":["**Question:** In many widely used datasets there are a lot of null entries. Propose 5 methods by which, one could deal with this problem. Briefly explain how do you decide which one to use in this problem."]},{"cell_type":"markdown","metadata":{"id":"l1u1pBHuAsSg"},"source":["**Your answer:**"]},{"cell_type":"markdown","metadata":{"id":"eHhH-hkpAxFf"},"source":["**Task:** Handle null entries using your best method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5fVwWcjK29fk"},"outputs":[],"source":["\n","# TODO: Determine the best method for handling null entries.\n","# Options include:\n","# - Dropping rows with null values using .dropna() if they are few in number.\n","# - Filling null values with a specific value (e.g., mean, median, or mode) using .fillna().\n","# - Consider more sophisticated methods, such as interpolation or model-based imputation, if appropriate.\n","\n","# TODO: Implement the chosen method to handle null entries in the dataset.\n","# For example, if dropping rows, use:\n","# data_cleaned = data.dropna()\n","\n","# TODO: If filling null values, decide on the strategy (mean, median, mode) for each relevant column.\n","# For example:\n","# data['column_name'].fillna(data['column_name'].mean(), inplace=True)\n","\n","# TODO: After handling null entries, verify that there are no more null values in the dataset.\n","# This can be done using the .isnull().sum() method again.\n","\n","# TODO: Print the results to confirm that null entries have been handled successfully."]},{"cell_type":"markdown","metadata":{"id":"43k5cTorCJaV"},"source":["**Task:** Convert categorical features to numerical values. Split the dataset with 80-20 portion. Normalize all the data using X_train. Use the built-in Logistic Regression function and GridSearchCV to train your model, and report the parameters, train and test accuracy of the best model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Agj18Lcd-vyZ"},"outputs":[],"source":["# TODO: Convert categorical features to numerical values using appropriate encoding methods.\n","# Options include one-hot encoding with pd.get_dummies() or label encoding using sklearn's LabelEncoder.\n","\n","# TODO: Split the dataset into features (X) and target (y).\n","# For example, set X to all columns except the target and y to the target column.\n","\n","# TODO: Use train_test_split from sklearn to split the dataset into training and testing sets with an 80-20 ratio.\n","# Set random_state for reproducibility.\n","\n","# TODO: Normalize the features using a method such as StandardScaler or MinMaxScaler.\n","# Fit the scaler on X_train and transform both X_train and X_test.\n","\n","# TODO: Import the LogisticRegression model and GridSearchCV from sklearn.\n","# Create an instance of LogisticRegression and specify the parameters you want to tune.\n","\n","# TODO: Define a parameter grid for GridSearchCV to search over.\n","# This should include hyperparameters such as 'C' (regularization strength), 'solver', etc.\n","\n","# TODO: Fit the GridSearchCV to the training data (X_train, y_train).\n","# Ensure you specify the scoring metric you want to optimize (e.g., accuracy).\n","\n","# TODO: Retrieve the best model and its parameters from GridSearchCV.\n","# Print the best parameters found by GridSearchCV.\n","\n","# TODO: Evaluate the best model on the training set and the test set.\n","# Calculate and print the training and testing accuracy.\n","\n","# TODO: Print the results to summarize the model performance."]},{"cell_type":"markdown","metadata":{"id":"6Lzr2lqXDQ1T"},"source":["**Task:** To try a different route, split X_train into $i$ parts, and train $i$ separate models on these parts. Now propose and implement 3 different *ensemble methods* to derive the global models' prediction for X_test using the results(not necessarily predictions!) of the $i$ models. Firstly, set $i=10$ to find the method with the best test accuracy(the answer is not general!). You must Use your own Logistic Regression model.(You might want to modify it a little bit for this part!)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K9D1jlstF9nF"},"outputs":[],"source":["# TODO: Set the value of i to 10 for splitting the training data into parts.\n","\n","# TODO: Split X_train into i parts. You can use numpy's array_split function or similar methods.\n","# This will create a list of subsets of X_train and the corresponding subsets of y_train.\n","\n","# TODO: Initialize a list to store the models trained on each part of the data.\n","\n","# TODO: For each part (subset) of the training data:\n","#       - Create an instance of your custom MyLogisticRegression model.\n","#       - Fit the model on the subset (X_part, y_part).\n","#       - Store the trained model in the list of models.\n","\n","# TODO: Define the three ensemble methods you want to implement.\n","\n","# TODO: Evaluate and compare the test accuracy of each ensemble method on X_test.\n","# Print the test accuracy for each ensemble method.\n","\n","# TODO: Based on the results, determine which ensemble method performed the best on the test set."]},{"cell_type":"markdown","metadata":{"id":"9QS9HYJ5FW1T"},"source":["**Question:** Explain your proposed methods and the reason you decided to use them!"]},{"cell_type":"markdown","metadata":{"id":"6hCBQuAeF46a"},"source":["**Your answer:**"]},{"cell_type":"markdown","metadata":{"id":"jjSREvg4FTHf"},"source":["**Task:** Now, for your best method, change $i$ from 2 to 100 and report $i$, train and test accuracy of the best model. Also, plot test and train accuracy for $2\\leq i\\leq100$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tfKS-Jq0-v4P"},"outputs":[],"source":["# TODO: Initialize lists to store the values of i, training accuracies, and testing accuracies.\n","\n","# TODO: Create a loop to iterate over the range of i from 2 to 100:\n","\n","# TODO: After the loop, determine the best value of i based on the test accuracy.\n","\n","# TODO: Print the best value of i, along with the corresponding training and test accuracy of the best model.\n","\n","# TODO: Plot the training and testing accuracies against i.\n","# Use a line plot to visualize the performance as i changes from 2 to 100.\n","# Make sure to label the axes and provide a title for the plot."]},{"cell_type":"markdown","metadata":{"id":"BWV0YUgRGg1p"},"source":["**Question:** Analyze the results."]},{"cell_type":"markdown","metadata":{"id":"8QzJTD9hVA4l"},"source":["**Your Answer:**"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}