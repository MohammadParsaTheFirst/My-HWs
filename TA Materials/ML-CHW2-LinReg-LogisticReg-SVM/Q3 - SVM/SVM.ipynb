{"cells":[{"cell_type":"markdown","metadata":{"id":"fL5WYF3BY3wB"},"source":["<h1 align=\"center\">An Introduction to Machine Learning - 25737</h1>\n","<h4 align=\"center\">Dr. Yassaee</h4>\n","<h4 align=\"center\">Sharif University of Technology, Autumn 2024</h4>\n","\n","**Student Name**:\n","\n","**Student ID**:"]},{"cell_type":"markdown","metadata":{"id":"TBq3yEPxSEWC"},"source":["# Non-Linear Classification with SVM"]},{"cell_type":"markdown","metadata":{"id":"xxGTRvPXTeDj"},"source":["## 1. DATA SET AND VISUALIZATION FUNCTION\n","We will use the non-linear toy data called the Moon dataset. You may use the code snippet below to generate the train/test set. Feel free to change the number of samples, and noise level. Additionally, a function plot_svm() is provided to help you visualize the decision boundary, margin, and support vectors on the dataset in 2D feature space.\n","\n","The provided function plot_svm() works out-the-box, and is the best way to visualize and evaluate the performance of your model. It assumes the classifier has an instance variable \"self.support_vectors_\", which is a numpy array of the support vectors found in training. DO NOT modify this function. Once your implementation in Task 3 is complete, the plots generated for your model should look similar to the plots generated for the standard library models in Task 2."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZAg_ZeuBY3wH"},"outputs":[],"source":["from sklearn.datasets import make_moons\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import precision_score, recall_score\n","from sklearn.svm import SVC\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.metrics import confusion_matrix, precision_score, recall_score\n","from scipy.stats import randint, uniform\n","import numpy as np\n","import cvxopt # The optimization package for Quadratic Programming\n","import cvxopt.solvers\n","from sklearn.base import BaseEstimator, ClassifierMixin"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dNLNqYtTY3wJ"},"outputs":[],"source":["# Generate a non-linear Moon dataset\n","X, y = make_moons(n_samples=500, noise=0.15, random_state=49)\n","\n","# Convert labels from {0,1} to {-1, +1}\n","# TODO: Convert the binary labels y from {0,1} to {-1, +1}\n","\n","# Split dataset into training and test sets\n","# TODO: Use train_test_split to split the data into train and test sets (80% training, 20% test) with random_state of 42.\n","\n","\n","def plot_svm(clf, X, y, axes=[-2, 3, -2, 2]):\n","    \"\"\"\n","    Generate a simple plot of SVM including the decision boundary, margin, and its training data\n","\n","    Parameters\n","    ----------\n","    clf: your classifier handle\n","    X: feature matrix shape(m_samples, n_features)\n","    y: label vector shape(m_samples, )\n","    axes: (optional) the axes of the plot in format [xmin, xmax, ymin, ymax]\n","    \"\"\"\n","    # TODO: Create a mesh grid for x0 and x1 axes based on the provided axes (100 x 100 resolution).\n","\n","\n","    # TODO: Combine the grid into a feature matrix (2-D points) and make predictions using clf for the mesh points.\n","\n","\n","\n","    # TODO: Set up the plot dimensions (e.g., 16x9).\n","\n","    # TODO: Plot data points for each class\n"]},{"cell_type":"markdown","metadata":{"id":"b_aV2q47DOdT"},"source":["- - -\n","## 2. TRAIN SVM FOR CLASSIFICATION TASK (20 pts)\n","\n","Use the standard libarary SVM classifier (SVC) on the training data, and then test the classifier on the test data. You will need to call SVM with 3 kernels: (1) Linear, (2) Polynomial and (3) Gaussian RBF.\n","\n","You should tune each model using a grid search or similar hyperparameter selection process, and report the best hyperparameters found. You will use these same hyperparameter settings later when testing and comparing to your implementation in Task 4. Once you've selected the best hyperparameters for each kernel, you will need to report the following:\n","\n","* Confusion matrix, Recall, and Precision. If applicable, discuss any tuning process on C and/or gamma to get to a reasonable result.\n","* Use the provided plot_svm() to visualize the SVM in 2D. This might give you some insight on how SVM determines the margin and support vector on the Moon dataset.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n5yxy7kgcvZx"},"outputs":[],"source":["def tuneAndTest(model, params):\n","  rnd_search = RandomizedSearchCV(model, param_distributions =params, n_iter = 50, cv = 5, random_state=40)\n","  rnd_search.fit(X_train, y_train)\n","  print(\"best hyper-parameter value: \", rnd_search.best_params_)\n","  bestmodel = rnd_search.best_estimator_\n","\n","  #predict\n","  prediction = bestmodel.predict(X_test)\n","  # print(\"model:\", str(bestmodel))\n","  print(\"confusion matrix:\", confusion_matrix(y_test, prediction))\n","  print(\"recall:\",recall_score(y_test, prediction))\n","  print(\"precision:\",precision_score(y_test, prediction))\n","  plot_svm(bestmodel, X_train, y_train)\n","  print(\"\\n\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2dApms_XY3wN"},"outputs":[],"source":["def tuneAndTest(model, params):\n","    \"\"\"\n","    Perform hyperparameter tuning using RandomizedSearchCV and evaluate the model on the test set.\n","\n","    Parameters:\n","    ----------\n","    model : the SVM model instance (SVC)\n","    params : the dictionary of hyperparameters to tune for this model\n","\n","    \"\"\"\n","    # TODO: Create RandomizedSearchCV instance with 50 iterations, 5-fold cross-validation, and a random state of 40.\n","\n","    # TODO: Fit the RandomizedSearchCV instance to X_train and y_train.\n","\n","    # TODO: Predict the labels for the test set (X_test).\n","\n","    # TODO: Print the confusion matrix for y_test vs. predicted values, along with recall and precision scores.\n","\n","    # Visualize decision boundary with plot_svm\n","    # TODO: Plot the SVM model using plot_svm function to visualize decision boundaries and support vectors.\n","\n","\n","# TODO: Define parameter grid for each kernel type (linear, polynomial, and RBF)\n","# Linear kernel: parameters for tuning C\n","\n","# Polynomial kernel: parameters for tuning C, degree, and coef0\n","\n","# RBF kernel: parameters for tuning C and gamma\n","\n","# Create pipelines for each kernel type\n","# TODO: Initialize SVC model with each kernel type in a separate pipeline with StandardScaler for scaling.\n","\n","# TODO: Call tuneAndTest function for each kernel type, passing in the respective SVM model and parameter grid.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IUHAntJGlwen"},"outputs":[],"source":["tuneAndTest(SVC(kernel=\"linear\"),{\"C\": uniform(0,20)})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"89QRZ8rQl1kr"},"outputs":[],"source":["tuneAndTest(SVC(kernel=\"poly\"),{\"coef0\":randint(low=0, high=5),\"C\": uniform(0,20),\"degree\": randint(low=2, high=5), \"gamma\":uniform(0, 1)})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4eH5Iqdvnmct"},"outputs":[],"source":["tuneAndTest(SVC(kernel=\"rbf\"),{\"C\": uniform(0,20), \"gamma\":uniform(0, 1)})"]},{"cell_type":"markdown","metadata":{"id":"ce_XKlbNZvXX"},"source":["## 3. IMPLEMENT YOUR OWN NON-LINEAR SVM (60 pts)\n","Now that you see how the standard library SVM perform on the dataset, you will attempt to implement your own version of SVM. To help you, a template of SVM has been created including the quadratic optimization. Essensially, you will get the optimized value of $\\alpha$ for free.\n","\n","The provided code is extensively documented in comments, so that you may write code compatible with it. DO NOT edit the provided code.\n","\n","Many of these tasks are made much easier by a working knowledge of numpy. If you have something you want to do, but are unsure how to do it in Python, consult the numpy documentation here: https://numpy.org/doc/1.17/reference/index.html. If you haven't built a Python class before, it may be worth scanning this tutorial: https://www.datacamp.com/community/tutorials/python-oop-tutorial."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ty2ssgfcw1E"},"outputs":[],"source":["# Use the information from the lecture slides to formulate the SVM Kernels.\n","# To help you get started, the Linear Kernel (simply just a dot product) has been provided to you.\n","# These kernel functions will be called in the SVM class\n","# Linear Kernel\n","def linear_kernel(u, v):\n","    return np.dot(u, v)\n","\n","# Polynomial Kernel (of degree up to and including p)\n","def polynomial_kernel(u, v, p=3):\n","    return np.dot(u,v)**p\n","\n","# Gaussian RBF Kernel\n","def rbf_kernel(u, v, gamma=0.1):\n","    # Note that gamma is provided, not sigma; see the slides for the relationship between gamma and sigma\n","    return np.exp(-gamma*np.linalg.norm(u-v)**2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ChnYAYaMY3wQ"},"outputs":[],"source":["class MySVM(BaseEstimator, ClassifierMixin):\n","    \"\"\"The Implementation of the SVM class\"\"\"\n","\n","    def __init__(self, kernel=linear_kernel, C=None):\n","        self.kernel = kernel  # Kernel function, can be called within the class\n","        self.C = C\n","        if self.C is not None:\n","            self.C = float(self.C)\n","\n","    def fit(self, X, y=None):\n","        \"\"\"\n","        Train SVM based on the training set\n","        Parameters\n","        ----------\n","        X: feature matrix shape(m_samples, n_features)\n","        y: label vector shape(m_samples, )\n","        \"\"\"\n","        self.X = X\n","        self.y = y\n","        m_samples, n_features = X.shape\n","\n","        # Compute the kernel matrix\n","        K = np.zeros((m_samples, m_samples))\n","        for i in range(m_samples):\n","            for j in range(m_samples):\n","                K[i, j] = self.kernel(X[i], X[j])\n","\n","        # Define Quadratic Programming parameters\n","        P = cvxopt.matrix(np.outer(y, y) * K)\n","        q = cvxopt.matrix(np.ones(m_samples) * -1)\n","        A = cvxopt.matrix(y, (1, m_samples))\n","        b = cvxopt.matrix(0.0)\n","\n","        # TODO: If self.C is None, set G and h for a hard margin SVM. Else, define them for soft margin SVM.\n","\n","        # Solve QP problem\n","        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n","        a = np.ravel(solution['x'])\n","\n","        # TODO: Identify support vectors and store them in self.support_vectors_\n","\n","        if self.kernel == linear_kernel:\n","            ay = (a * y)[:, np.newaxis]\n","            self.w = np.dot(ay.T, X).T\n","        else:\n","            # No weight vector needed for non-linear case.\n","            self.w = None\n","\n","        # TODO: Compute the intercept b based on support vectors.\n","\n","    def decision_function(self, X):\n","        \"\"\"The decision function is essentially w^T . x + b\"\"\"\n","        if self.w is not None:\n","            return np.dot(X, self.w) + self.b\n","        else:\n","            # TODO: Implement the kernel trick to predict label for non-linear SVM\n","\n","    def predict(self, X):\n","        \"\"\"Predicts -1,+1 based on the sign of the decision function\"\"\"\n","        return np.sign(self.decision_function(X))\n"]},{"cell_type":"markdown","metadata":{"id":"TuI0EfvTJTIH"},"source":["---\n","## 4. COMPARE YOUR IMPLEMENTATION TO THE STANDARD LIBRARY (20 pts)\n","Now that you have implemented your own SVM class, let's use it! Create 3 instances of your SVM class, each with a difference kernel (Linear, Polynomial, and RBF kernel), then train and test its performance in the Moon dataset as above. Use the same hyperparameters found to be best for the standard library implementations, and report the same metrics (confusion matrix, recall, and precision). You can use the plot_svm() function to visualize your SVM with decision boundary, margin, and support vectors on the dataset, and should plot each of the three instances of your class, to compare these plots with the standard library models' plots.\n","\n","Based on the number above, compare your SVM implementation with the standard library version. How did your SVM perform in comparison? Is there any major differences between the algorithms? If your performance is significantly worse, is there a different set of hyperparameters which better fits your model? Finally, reflect on your experience implementing a learning algorithm for this assignment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R6cw27RRY3wQ"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def plot_svm(clf, X, y, axes=[-2, 3, -2, 2]):\n","    \"\"\"\n","    Visualizes the SVM decision boundary, margins, and support vectors.\n","    \"\"\"\n","    # TODO: Set up a mesh grid over the specified axes to visualize decision boundaries\n","\n","    # TODO: Flatten the mesh grid to feed into clf.predict and clf.decision_function for visualization\n","\n","    # Plotting\n","    plt.figure(figsize=(16, 9))\n","    plt.plot(X[:, 0][y == -1], X[:, 1][y == -1], \"bo\", label=\"Class -1\")\n","    plt.plot(X[:, 0][y == 1], X[:, 1][y == 1], \"go\", label=\"Class +1\")\n","\n","    # TODO: Use clf.support_vectors_ to plot support vectors\n","\n","    # TODO: Use contour plotting to show the decision boundary and margins\n","\n","\n","def printResult(model):\n","    \"\"\"Displays confusion matrix, recall, precision, and plots SVM boundaries.\"\"\"\n","    # TODO: Use model.predict() to generate predictions for the test set\n","\n","    # TODO: Print out evaluation metrics (confusion matrix, recall, precision) for the model\n","\n","    # TODO: Use plot_svm to visualize the decision boundary, support vectors, and margins\n","\n","\n","def comp(mysvm, lib):\n","    \"\"\"Compares the custom SVM with the standard library SVM implementation.\"\"\"\n","    # TODO: Fit both the custom SVM and library SVC model on the training data\n","\n","    print(\"Custom SVM Implementation:\")\n","    print(mysvm)\n","    # TODO: Call printResult to display the results of the custom SVM\n","\n","    print(\"Standard Library Implementation:\")\n","    # TODO: Call printResult to display the results of the library SVC model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9N5fI0a0Y3wR"},"outputs":[],"source":["custom_linear_svm = MySVM(kernel=linear_kernel, C=8.1537)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-zvLfTD6Y3wR"},"outputs":[],"source":["comp(custom_linear_svm, SVC(kernel=\"linear\", C=8.1537))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h_t2tTMhY3wR"},"outputs":[],"source":["custom_poly_svm = MySVM(kernel=lambda u, v: polynomial_kernel(u, v, p=3), C=0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ie4hkW8BY3wR"},"outputs":[],"source":["comp(custom_poly_svm, SVC(kernel=\"poly\", C=0.5, degree=3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DHZkLKwWY3wR"},"outputs":[],"source":["custom_rbf_svm = MySVM(kernel=lambda u, v: rbf_kernel(u, v, gamma=0.1), C=1.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1gm6HhVhY3wS"},"outputs":[],"source":["comp(custom_rbf_svm, SVC(kernel=\"rbf\", C=1.5, gamma=0.1))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}